# Scrapy教程-采集你的第一个网页



## Scrapy采集

Scrapy官方网站上找到文档：https://docs.scrapy.org/

Scrapy官方GitHub库：Scrapy的官方GitHub库包含了源代码、问题追踪和讨论，你可以在这里找到一些有用的资源和示例：https://github.com/scrapy/scrapy



## 示例1: 爬取网站

这里以爬取一个简单的网站为例。假设我们要爬取url为http://quotes.toscrape.com/的网站。

```
$ scrapy startproject myproject
$ cd myproject
$ scrapy genspider quotes_spider quotes.toscrape.com
```

在myproject/spiders目录中会生成一个quotes_spider.py文件。打开这个文件，加入如下代码：

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```

运行如下命令：

```
$ scrapy crawl quotes
```

Scrapy会下载http://quotes.toscrape.com/page/1/和http://quotes.toscrape.com/page/2/上的网页，并将网页保存到当前目录下的quotes-1.html和quotes-2.html文件中。



## 示例2: 爬取Amazon网站

这里以爬取Amazon网站为例。假设我们要爬取url为https://www.amazon.com的网站。

```
$ scrapy startproject myproject2
$ cd myproject2
$ scrapy genspider amazon_spider amazon.com
```

在myproject/spiders目录中会生成一个amazon_spider.py文件。打开这个文件，加入如下代码：

```python
import scrapy

class AmazonSpider(scrapy.Spider):
    name = "amazon"
    allowed_domains = ["amazon.com"]
    start_urls = [
        "https://www.amazon.com/books-used-books-textbooks/b/ref=nav_shopall_bo_t3?ie=UTF8&node=283155"
    ]

    def parse(self, response):
        for sel in response.xpath('//ul[@class="s-result-list s-col-3"]/li'):
            title = sel.xpath('div/div[2]/div[@class="a-row"]/a/@title').extract_first()
            price = sel.xpath('div/div[4]/div[1]/a/span/text()').extract_first()
            if title is not None and price is not None:
                yield {
                    'title': title,
                    'price': price
                }

        next_page_url = response.xpath('//a[text()="Next"]').extract_first()
        if next_page_url is not None:
            yield scrapy.Request(response.urljoin(next_page_url), callback=self.parse)
```

运行如下命令：

```
$ scrapy crawl amazon
```



Scrapy会爬取Amazon网站上的二手书的标题和价格，并将这些信息保存到命令行中。如果Amazon网站上有多页结果，Scrapy会继续翻页并继续爬取。

